import hashlib
import gradio as gr
import os
from PIL import Image
import requests
import base64

os.environ['GRADIO_TEMP_DIR'] = os.path.expanduser('./tmp')

# Sample preloaded example images
API_URL = "http://localhost:8314/predict"
EXAMPLE_IMAGES = [
    "https://raw.githubusercontent.com/gradio-app/gradio/main/test/test_files/bus.png",
    "https://raw.githubusercontent.com/gradio-app/gradio/main/test/test_files/cheetah1.jpg",
]

EXAMPLE_IMAGES_DICT = [
    {'images_1': './examples/sample/test_1.png', 'message':'·∫¢nh ch·ª•p X-ray PA (Ch·ª•p Xquang tim ph·ªïi th·∫≥ng) b·ªánh nh√¢n nam, 53 tu·ªïi. Cho bi·∫øt b·ªánh nh√¢n b·ªã g√¨?'},
    {'images_2': './examples/sample/test_2.png', 'message':'·∫¢nh ch·ª•p X-ray PA (Ch·ª•p Xquang tim ph·ªïi th·∫≥ng) b·ªánh nh√¢n nam, 35 tu·ªïi. Cho bi·∫øt b·ªánh nh√¢n b·ªã g√¨?'},
    {'images_3': './examples/sample/test_3.png', 'message':'·∫¢nh ch·ª•p X-ray PA (Ch·ª•p Xquang tim ph·ªïi th·∫≥ng) b·ªánh nh√¢n nam, 61 tu·ªïi. Cho bi·∫øt b·ªánh nh√¢n b·ªã g√¨?'},
    {'images_4': './examples/sample/test_4.png', 'message':' ·∫¢nh ch·ª•p X-ray PA (Ch·ª•p Xquang tim ph·ªïi th·∫≥ng) b·ªánh nh√¢n nam, 79 tu·ªïi. Cho bi·∫øt b·ªánh nh√¢n b·ªã g√¨?'},
    {'images_5': './examples/sample/test_5.png', 'message':'Ch·ª•p Xquang tim ph·ªïi th·∫≥ng) b·ªánh nh√¢n n·ªØ, 33 tu·ªïi. Cho bi·∫øt b·ªánh nh√¢n b·ªã g√¨?'},
    
]


def api_run(image, model_number, messages= None):
    """API call function for different models"""
    if image is None:
        return "Please upload an image first."
    
    # Map model number to model name
    model_mapping = {
        1: "clara",
        2: "gemini",
        3: "gpt"
    }
    
    model_name = model_mapping.get(model_number, "clara")
    
    os.makedirs("output", exist_ok=True)
    # Hash image content
    hash_image = hashlib.md5(image.tobytes()).hexdigest()
    image.save(os.path.join("output", f"{hash_image}.png"))
    
    # Convert image to base64
    with open(f"output/{hash_image}.png", "rb") as img_file:
        encoded_image = base64.b64encode(img_file.read()).decode("utf-8")
    
    if messages is None:
        messages = ' "·∫¢nh X-quang n√†y c√≥ g√¨ b·∫•t th∆∞·ªùng?"'
        
        
    
    # Prepare request payload
    payload = {
        "images": encoded_image,
        "text": messages,
        "model_name": model_name
    }
    
    try:
        response = requests.post(API_URL, json=payload)
        if response.status_code == 200:
            return response.json()["outputs"]
        else:
            return f"‚ùå Error: {response.status_code} - {response.text}"
    except Exception as e:
        return f"‚ùå Connection Error: {str(e)}"

# def load_example_image(selection: gr.SelectData):
#     """Load example image from gallery"""
#     return EXAMPLE_IMAGES[selection.index]

css = """
.output-box {
    border: 2px solid #888;
    border-radius: 10px;
    padding: 15px;
    background-color: #1e1e2f;  /* n·ªÅn t·ªëi nh·∫π n·∫øu giao di·ªán dark mode */
    margin-top: 10px;
}
"""


with gr.Blocks(title="Multi-Modal AI Assistant", theme=gr.themes.Soft(), css=css) as demo:
    gr.Markdown("# ü§ñ Multi-Modal AI Assistant")
    gr.Markdown("Upload an image and get responses from different AI models!")

    messages = gr.State("")

    with gr.Row():
        with gr.Column(scale=1):
            image_input = gr.Image(label="üì∑ Upload Image", type="pil", height=300)

            gr.Markdown("### üñºÔ∏è Example Cases")
            gr.Markdown("Click any example below to load it:")

            example_gallery = gr.Gallery(
                value=[next(v for k, v in example.items() if k.startswith("images_"))
                       for example in EXAMPLE_IMAGES_DICT],
                label="Click an example to load image and message",
                show_label=True,
                elem_id="gallery",
                columns=2,
                rows=3,
                height="auto",
                allow_preview=False
            )

        with gr.Column(scale=2):
            with gr.Tabs():
                with gr.TabItem("Clara"):
                    output_1 = gr.Markdown(label="Model Response", value="", show_label=True,
                                           elem_id="clara-output", elem_classes=["output-box"])
                    submit_1 = gr.Button("Send", scale=1, variant="primary")

                with gr.TabItem("Gemini"):
                    output_2 = gr.Markdown(label="Model Response", value="", show_label=True,
                                           elem_id="gemini-output", elem_classes=["output-box"])
                    submit_2 = gr.Button("Send", scale=1, variant="primary")

                with gr.TabItem("ChatGPT"):
                    output_3 = gr.Markdown(label="Model Response", value="", show_label=True,
                                           elem_id="chatgpt-output", elem_classes=["output-box"])
                    submit_3 = gr.Button("Send", scale=1, variant="primary")

    def load_example(evt: gr.SelectData):
        selected_example = EXAMPLE_IMAGES_DICT[evt.index]
        image_path = next(v for k, v in selected_example.items() if k.startswith("images_"))
        message = selected_example["message"]
        return image_path, message

    example_gallery.select(
        fn=load_example,
        outputs=[image_input, messages]
    )

    # Submit button events with clear output
    submit_1.click(
        fn=lambda img, msg: api_run(img, 1, msg),
        inputs=[image_input, messages],
        outputs=output_1
    ).then(fn=lambda: "", outputs=messages)

    submit_2.click(
        fn=lambda img, msg: api_run(img, 2, msg),
        inputs=[image_input, messages],
        outputs=output_2
    ).then(fn=lambda: "", outputs=messages)

    submit_3.click(
        fn=lambda img, msg: api_run(img, 3, msg),
        inputs=[image_input, messages],
        outputs=output_3
    ).then(fn=lambda: "", outputs=messages)



if __name__ == "__main__":
    demo.launch(share=True, debug=True)